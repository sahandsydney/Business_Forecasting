{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as req\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Status quo will not be disturbed at Ayodhya; s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Fissures in Hurriyat over Pak visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>America's unwanted heading for India?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>For bigwigs; it is destination Goa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20010102</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Extra buses to clear tourist traffic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date headline_category   \n",
       "0      20010102           unknown  \\\n",
       "1      20010102           unknown   \n",
       "2      20010102           unknown   \n",
       "3      20010102           unknown   \n",
       "4      20010102           unknown   \n",
       "\n",
       "                                       headline_text  \n",
       "0  Status quo will not be disturbed at Ayodhya; s...  \n",
       "1                Fissures in Hurriyat over Pak visit  \n",
       "2              America's unwanted heading for India?  \n",
       "3                 For bigwigs; it is destination Goa  \n",
       "4               Extra buses to clear tourist traffic  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"..\\data\\india_news_raw.csv\")\n",
    "news.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['adani','port business','cargo','ministry of ports','waterways',\"mundra port\", \"krishnapatnam\", \"hazira\",\n",
    "\"dhamra\",\n",
    "\"dahej\",\n",
    "\"vizag\",\n",
    "\"mormugao\",\n",
    "\"vizhinjam\",\n",
    "\"kattupalli\",\n",
    "\"kamarajar\",\n",
    "\"tuna port\",\n",
    "\"agardana\",\n",
    "\"dighi\"]\n",
    "news = news[news['headline_text'].str.contains('|'.join(keywords))]\n",
    "news['headline_text'] = news['headline_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6059</td>\n",
       "      <td>20010625</td>\n",
       "      <td>city.delhi</td>\n",
       "      <td>rs 8 cr cargo fraud detected at igi airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17755</td>\n",
       "      <td>20010821</td>\n",
       "      <td>home.science</td>\n",
       "      <td>russian cargo ship heads to iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29229</td>\n",
       "      <td>20010917</td>\n",
       "      <td>cricket</td>\n",
       "      <td>badani replaces injured laxman for sa tour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29285</td>\n",
       "      <td>20010917</td>\n",
       "      <td>cricket</td>\n",
       "      <td>badani ready to win his place back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36431</td>\n",
       "      <td>20011010</td>\n",
       "      <td>city.delhi</td>\n",
       "      <td>fewer cargo planes prolong elephant's stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>3645637</td>\n",
       "      <td>20220321</td>\n",
       "      <td>city.kolkata</td>\n",
       "      <td>kol port's bangla link with ship-to-ship lpg c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>3646740</td>\n",
       "      <td>20220323</td>\n",
       "      <td>city.ahmedabad</td>\n",
       "      <td>apsez handled 300mmt cargo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>3647704</td>\n",
       "      <td>20220325</td>\n",
       "      <td>city.hyderabad</td>\n",
       "      <td>air cargo saw massive surge in demand: industr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>3647725</td>\n",
       "      <td>20220325</td>\n",
       "      <td>city.kolkata</td>\n",
       "      <td>bangladeshi cargo ship capsizes at kolkata port</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>3648281</td>\n",
       "      <td>20220326</td>\n",
       "      <td>city.ranchi</td>\n",
       "      <td>jharkhand: 5 feared drowned as cargo vessel lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1173 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  publish_date headline_category   \n",
       "0        6059      20010625        city.delhi  \\\n",
       "1       17755      20010821      home.science   \n",
       "2       29229      20010917           cricket   \n",
       "3       29285      20010917           cricket   \n",
       "4       36431      20011010        city.delhi   \n",
       "...       ...           ...               ...   \n",
       "1168  3645637      20220321      city.kolkata   \n",
       "1169  3646740      20220323    city.ahmedabad   \n",
       "1170  3647704      20220325    city.hyderabad   \n",
       "1171  3647725      20220325      city.kolkata   \n",
       "1172  3648281      20220326       city.ranchi   \n",
       "\n",
       "                                          headline_text  \n",
       "0           rs 8 cr cargo fraud detected at igi airport  \n",
       "1                       russian cargo ship heads to iss  \n",
       "2            badani replaces injured laxman for sa tour  \n",
       "3                    badani ready to win his place back  \n",
       "4            fewer cargo planes prolong elephant's stay  \n",
       "...                                                 ...  \n",
       "1168  kol port's bangla link with ship-to-ship lpg c...  \n",
       "1169                         apsez handled 300mmt cargo  \n",
       "1170  air cargo saw massive surge in demand: industr...  \n",
       "1171    bangladeshi cargo ship capsizes at kolkata port  \n",
       "1172  jharkhand: 5 feared drowned as cargo vessel lo...  \n",
       "\n",
       "[1173 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.reset_index(inplace=True)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [WinError\n",
      "[nltk_data]     10061] No connection could be made because the target\n",
      "[nltk_data]     machine actively refused it>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2001, Sentiment Score: 1\n",
      "Year: 2002, Sentiment Score: 1\n",
      "Year: 2003, Sentiment Score: 3\n",
      "Year: 2004, Sentiment Score: 2\n",
      "Year: 2005, Sentiment Score: 0\n",
      "Year: 2006, Sentiment Score: -2\n",
      "Year: 2007, Sentiment Score: 3\n",
      "Year: 2008, Sentiment Score: 9\n",
      "Year: 2009, Sentiment Score: -5\n",
      "Year: 2010, Sentiment Score: 1\n",
      "Year: 2011, Sentiment Score: -9\n",
      "Year: 2012, Sentiment Score: -3\n",
      "Year: 2013, Sentiment Score: 4\n",
      "Year: 2014, Sentiment Score: 2\n",
      "Year: 2015, Sentiment Score: 3\n",
      "Year: 2016, Sentiment Score: 10\n",
      "Year: 2017, Sentiment Score: 4\n",
      "Year: 2018, Sentiment Score: 15\n",
      "Year: 2019, Sentiment Score: 2\n",
      "Year: 2020, Sentiment Score: 0\n",
      "Year: 2021, Sentiment Score: 9\n",
      "Year: 2022, Sentiment Score: 8\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the news dataset into a Pandas DataFrame\n",
    "data = news\n",
    "\n",
    "# Apply sentiment analysis to each headline and assign sentiment scores\n",
    "data['NLTK'] = data['headline_text'].apply(lambda x: 1 if sia.polarity_scores(x)['compound'] > 0.1 else (-1 if sia.polarity_scores(x)['compound'] < -0.1 else 0))\n",
    "\n",
    "# Extract the year from the publish_date column\n",
    "data['year'] = data['publish_date'].astype(str).str[:4]\n",
    "\n",
    "# Group the data by year and calculate the sum of sentiment scores for each year\n",
    "yearly_sentiment_scores = data.groupby('year')['NLTK'].sum().to_dict()\n",
    "\n",
    "# Display the sentiment scores year-wise\n",
    "for year, sentiment_score in yearly_sentiment_scores.items():\n",
    "    print(f\"Year: {year}, Sentiment Score: {sentiment_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>year</th>\n",
       "      <th>NLTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20010720</td>\n",
       "      <td>city.ahmedabad</td>\n",
       "      <td>DRI seizes Adani HSD stock</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20011005</td>\n",
       "      <td>city.ahmedabad</td>\n",
       "      <td>I-T raids on shipping units</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20011029</td>\n",
       "      <td>business.india-business</td>\n",
       "      <td>India's PC shipment dips 5%</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20011030</td>\n",
       "      <td>city.mumbai</td>\n",
       "      <td>I-T raids offices of shipping agencies</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20011109</td>\n",
       "      <td>city.pune</td>\n",
       "      <td>First Divgi-Warner shipment leaves for China</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>20220323</td>\n",
       "      <td>city.ahmedabad</td>\n",
       "      <td>Adani Power inks MoU with IHI Corp; Korean firm</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>20220328</td>\n",
       "      <td>auto.cars</td>\n",
       "      <td>Adani Total forays into electric mobility infr...</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>20220328</td>\n",
       "      <td>city.ahmedabad</td>\n",
       "      <td>Adani Total Gas forays into EV segment</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>20220329</td>\n",
       "      <td>city.ahmedabad</td>\n",
       "      <td>Adani Group inks multi-year partnership with G...</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>20220330</td>\n",
       "      <td>city.mumbai</td>\n",
       "      <td>Adani Group completes financial closure of Nav...</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1334 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      publish_date        headline_category   \n",
       "0         20010720           city.ahmedabad  \\\n",
       "1         20011005           city.ahmedabad   \n",
       "2         20011029  business.india-business   \n",
       "3         20011030              city.mumbai   \n",
       "4         20011109                city.pune   \n",
       "...            ...                      ...   \n",
       "1329      20220323           city.ahmedabad   \n",
       "1330      20220328                auto.cars   \n",
       "1331      20220328           city.ahmedabad   \n",
       "1332      20220329           city.ahmedabad   \n",
       "1333      20220330              city.mumbai   \n",
       "\n",
       "                                          headline_text  year  NLTK  \n",
       "0                            DRI seizes Adani HSD stock  2001     0  \n",
       "1                           I-T raids on shipping units  2001     0  \n",
       "2                           India's PC shipment dips 5%  2001     0  \n",
       "3                I-T raids offices of shipping agencies  2001     0  \n",
       "4          First Divgi-Warner shipment leaves for China  2001     0  \n",
       "...                                                 ...   ...   ...  \n",
       "1329    Adani Power inks MoU with IHI Corp; Korean firm  2022     0  \n",
       "1330  Adani Total forays into electric mobility infr...  2022     0  \n",
       "1331             Adani Total Gas forays into EV segment  2022     0  \n",
       "1332  Adani Group inks multi-year partnership with G...  2022     0  \n",
       "1333  Adani Group completes financial closure of Nav...  2022     0  \n",
       "\n",
       "[1334 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#news = news.drop(\"sentiment_score\", axis=1)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news.to_csv(\"..\\data\\sentiment_news.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2001, Sentiment Score: 1\n",
      "Year: 2002, Sentiment Score: 5\n",
      "Year: 2003, Sentiment Score: 4\n",
      "Year: 2004, Sentiment Score: 3\n",
      "Year: 2005, Sentiment Score: 0\n",
      "Year: 2006, Sentiment Score: -2\n",
      "Year: 2007, Sentiment Score: 3\n",
      "Year: 2008, Sentiment Score: 12\n",
      "Year: 2009, Sentiment Score: -2\n",
      "Year: 2010, Sentiment Score: 5\n",
      "Year: 2011, Sentiment Score: -5\n",
      "Year: 2012, Sentiment Score: 2\n",
      "Year: 2013, Sentiment Score: 11\n",
      "Year: 2014, Sentiment Score: 8\n",
      "Year: 2015, Sentiment Score: 14\n",
      "Year: 2016, Sentiment Score: 15\n",
      "Year: 2017, Sentiment Score: 14\n",
      "Year: 2018, Sentiment Score: 20\n",
      "Year: 2019, Sentiment Score: 8\n",
      "Year: 2020, Sentiment Score: 6\n",
      "Year: 2021, Sentiment Score: 17\n",
      "Year: 2022, Sentiment Score: 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the news dataset into a Pandas DataFrame\n",
    "data = news  # Assuming you have loaded the news dataset into the 'news' variable\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the sentiment classification model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Tokenize and encode the headlines for sentiment analysis using BERT\n",
    "encoded_inputs = tokenizer(data['headline_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Perform sentiment analysis using BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Assign sentiment scores based on BERT predictions\n",
    "data['BERT_sentiment_score'] = predictions.tolist()\n",
    "\n",
    "# Apply sentiment analysis to each headline and assign sentiment scores using SentimentIntensityAnalyzer\n",
    "data['VADER_sentiment_score'] = data['headline_text'].apply(lambda x: 1 if sia.polarity_scores(x)['compound'] > 0.1 else (-1 if sia.polarity_scores(x)['compound'] < -0.1 else 0))\n",
    "\n",
    "# Combine the sentiment scores from BERT and VADER to get the final sentiment score\n",
    "data['final_sentiment_score'] = data['BERT_sentiment_score'] + data['VADER_sentiment_score']\n",
    "\n",
    "# Extract the year from the publish_date column\n",
    "data['year'] = data['publish_date'].astype(str).str[:4]\n",
    "\n",
    "# Group the data by year and calculate the sum of final sentiment scores for each year\n",
    "yearly_sentiment_scores = data.groupby('year')['final_sentiment_score'].sum().to_dict()\n",
    "\n",
    "# Display the sentiment scores year-wise\n",
    "for year, sentiment_score in yearly_sentiment_scores.items():\n",
    "    print(f\"Year: {year}, Sentiment Score: {sentiment_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 4s 23ms/step - loss: 0.6850 - accuracy: 0.5832\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 0.6707 - accuracy: 0.5832\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 1s 19ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.6706 - accuracy: 0.5832\n",
      "42/42 [==============================] - 1s 11ms/step\n",
      "Year: 2001, Sentiment Score: -6\n",
      "Year: 2002, Sentiment Score: -23\n",
      "Year: 2003, Sentiment Score: -17\n",
      "Year: 2004, Sentiment Score: -8\n",
      "Year: 2005, Sentiment Score: -4\n",
      "Year: 2006, Sentiment Score: -20\n",
      "Year: 2007, Sentiment Score: -24\n",
      "Year: 2008, Sentiment Score: -28\n",
      "Year: 2009, Sentiment Score: -52\n",
      "Year: 2010, Sentiment Score: -55\n",
      "Year: 2011, Sentiment Score: -58\n",
      "Year: 2012, Sentiment Score: -89\n",
      "Year: 2013, Sentiment Score: -63\n",
      "Year: 2014, Sentiment Score: -147\n",
      "Year: 2015, Sentiment Score: -120\n",
      "Year: 2016, Sentiment Score: -99\n",
      "Year: 2017, Sentiment Score: -140\n",
      "Year: 2018, Sentiment Score: -82\n",
      "Year: 2019, Sentiment Score: -77\n",
      "Year: 2020, Sentiment Score: -76\n",
      "Year: 2021, Sentiment Score: -111\n",
      "Year: 2022, Sentiment Score: -35\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the news dataset into a Pandas DataFrame\n",
    "data = news\n",
    "\n",
    "# Apply sentiment analysis to each headline and assign sentiment scores\n",
    "data['sentiment_score'] = data['headline_text'].apply(lambda x: 1 if sia.polarity_scores(x)['compound'] > 0.1 else (-1 if sia.polarity_scores(x)['compound'] < -0.1 else 0))\n",
    "\n",
    "# Extract the year from the publish_date column\n",
    "data['year'] = data['publish_date'].astype(str).str[:4]\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['headline_text'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(data['headline_text'])\n",
    "max_length = max(len(sequence) for sequence in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_length))\n",
    "model.add(LSTM(128, activation='tanh'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X, data['sentiment_score'], epochs=10, batch_size=32)\n",
    "\n",
    "# Perform sentiment analysis using the trained LSTM model\n",
    "predicted_probs = model.predict(X)\n",
    "predicted_scores = [1 if prob > 0.5 else -1 for prob in predicted_probs]\n",
    "\n",
    "# Assign sentiment scores based on predictions\n",
    "data['LSTM_sentiment_score'] = predicted_scores\n",
    "\n",
    "# Group the data by year and calculate the sum of sentiment scores for each year\n",
    "yearly_sentiment_scores = data.groupby('year')['LSTM_sentiment_score'].sum().to_dict()\n",
    "\n",
    "# Display the sentiment scores year-wise\n",
    "for year, sentiment_score in yearly_sentiment_scores.items():\n",
    "    print(f\"Year: {year}, Sentiment Score: {sentiment_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m news\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news' is not defined"
     ]
    }
   ],
   "source": [
    "news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
